It should hardly come as a surprise that code/software lies as a mediator between ourselves and our corporeal experiences, disconnecting the physical world from a direct coupling with our physicality, while managing a looser softwarized transmission system. Called 'fly-by-wire' in aircraft design, in reality fly-by-wire is the condition of the computational environment we increasingly experience, hence the term computationality (Berry 2011a)


Computational experience within capitalism is constitutive of this ontology. p.97
* Reading Notes

* Counter-culture to Cyberculture
** 176
	 On the development of the GBN (global business network) - Within the precinct of the Global Business Network and _Out of Control_, however, these organizations acquired a new political valence. They became models of a collaborative world, a world in which technologies were rendering information systems visible, material production processes irrelevant, and bureaucracy obsolete. Their executives and engineers also became visible stand-ins for the emerging corporate elect- entrepreneurial, technologically savvy, but socially and culturally conservative. Like the Communards of New Mexico and northern California, the scientists, futurists and entrepreneurs of the Global Business Network and _Out of Control_ constituted a highly educated, mobile and predominantly white elite.
** 180
	 Negroponte - "Some of us enjoy a privileged existence where our work life and our leisure life are almost synonymous, he told Brand. "More and more people I think can move into that position with the coming of truly intimate technology
** 204
	 In the pages of _Out of Control_the lone hunter of the _Whole Earth Catalog_ had become and entrepreneur... Almost thirty years earlier, thousands of young, highly educated Americans had tromped off into the wilderness seeking to build and egalitarian, fun-loving world. Today, suggested Kelly, they should look to technology and the economy for satisfaction...

	 They should do so, he argued, because the world itself was an information system. In his view, to manipulate computers and to manipulate information was not simply to hold down a job; it was to gain access to a  hidden world, to live by its laws, and to become in a sense a Comprehensive Designer of one's own fate. In that sense, Kelly's vision echoed the New Communalists' celebration of consciousness. It also resuscitated the commune dwellers' disregard for the demands of the material world. In the 1960s, many had set out for rural America with little sense of the embodied labor that building their new society might take and little feel for the work already done by those among who they settled. In the 1990s, as a  number of critics have noted, Kelly's doctrine of cyberrevolutionism gave a potent ideological boost to executives seeking to out-source labor, automate industrial processes, and decrease the stability of these workers employment. Throughout his book, Kelly underplayed the work of embodied labor, celebrated intellect and collaborative styles associated with intellectual institutions, and so offered a model of a world inhabited exclusively by free-lancing elites. In the early 1990s, as in the late 1960s, that turn away from the material world helped legitimate the authority of those who controlled information and information systems by rendering invisible those who did not.

	 At the same time, the turn toward imagining the world in terms of dematerialized networks of information helped assuage the increasing sense of helplessness among executives themselves. On the one hand, like scientists at the Rad Lab a half century earlier, executives could call on the rhetoric of cybernetics to justify the pursuit of their professional goals. Like the cold warriors who had long ago scanned their computer screens for signs of incoming bombers, they could imagine the world as an information system and themselves as monitors of that system. Thanks to the computers they were so rapidly installing in their firms, they could see farther, plan more effectively, and perhaps manage their firms "as gods". On the other hand, they could begin to accept that they lacked precisely the sorts of power that Stewart Brand and others of his generation imagined belonged to corporate leaders. In the late 1980s and early 1990s, despite the spike in executive incomes and the broad-based movement of wealth towards those at the top of the social ladder, many executives labored under a sense that they were surrounded by forces ~beyond~ their control. ... "the missing ingredient in biology... is values"

** 219
	 Quittner summed up her argument this way: "The Net, the very network itself, is merely a means to an end. The end is to reverse-engineer government, to hack Politics down to its component parts and fix it."
	 The urge to "hack" politics by bringing governance down to a manageable local level and by basing social integration on technologically facilitated forms of consciousness was one of the driving impulses behind the New Communalist movement.
** 224
** 229
** =237=
		their techno-utopian social vision in fact reflected the slow entwining of two far deeper transformation in American society. The first of these was technological. Over the previous forty years, the massive, stand alone calculating machines of the cold war had become desktop computers, linked to one another in a vast network of communications that reached into almost every corner of the civilian world. This shift in computing technology took place, however, alongside a second, cultural transformation. In the late 1950s, Stewart Brand and others of his generation had come of age fearing that they would soon be absorbed into an unfeeling...

=By the late 1990s, Brand and his Whole Earth colleagues had repeatedly linked these technological and cultural changes and in the process had helped turn the terms of their generational search into the key frames by which the American public understood the social possibilities of computers and computer networking... desktop computers had come to be seen as "personal" technology. In keeping with the New Communalist ethos of tool use, they promised to transform the individual consciousness and society at large. Thanks to the citizens of the WELL, computer-mediated communication had been re-imagined in terms of disembodied, communal harmony and renamed virtual community. Cyberspace itself had been re-imagined as an electronic frontier. Finally, in the 1990s, the social and professional networks o the Global Business Network and Wired seemed to suggest that a new, networked form of economic life was emerging. Because of computer technologies, their example implied, it was finally becoming possible to move through life not in heirachical bureaucratic towers, but as member of flexible, temporary, and culturally congenial tribes.

In all of these ways, members of the Whole Earth network helped reverse the political valence of information and information technology and turn computers into emblems of counter-cultural revolution. At the same time, however, they legitimated a metamorphosis within- and a widespread diffusion of- the core cultural styles of the military-industrial-academic technocracy that their generation had sought to undermine. In the imagination of the young Stewart Brand and others like him, the middle-aged men who ran the corporations, universities, and governments of the cold war had found themselves locked into rigid roles... throughout the military-industrial-academic complex responsible for developing America's defense technologies, a far more collaborative style was emerging.

.. Stewart Brand and the Whole Earth network not only reconfigured the cultural status of information and information technologies as they moved from the government-funded, military-industrial research world into society at large; they also helped legitimate a parallel migration on the part of that world's cultural style. Moreover, they did so by embracing the cybernetic theories of information, the universal rhetorical techniques,a and the flexible social practices born out the interdisciplinary collaborations of World War II. Like the designers of that era's weapons research laboratories, Brand and his colleagues created network forums in which members of multiple coial and technical communities could come together, collaborate, and , in the process, build shared understandings of their collective interests. Expressed first in local contact languages, these understandings were repeatedly exported from the forums themselves, either by forum members or by professional journalists in attendance. Like the laboratories that first gave rise to cybernetics, however, the forums produced more than new bits of rhetoric. They also produced new social networks nad, in Brand's case, new information systems, such as catalogs, meetings, and online gatherings. These sytems in turn hosted and helped to create new social and professional networks and at the same time modeled teh networks' governing ideals.=
** 259
	 Problems of homogenaity in leading this life...
** =259-260=

	 in keeping with the vision's history as the universal rehorical tool with which cold war researchers claimed authority for their projects, the fact tht the social and the natural, the individual tand the institutional, the human and the machine could all be seen as refelctions of one another suggested that those who could most successfully decict themselves as aligned witht the forces of information could alos claim to be models of thoise forces. They could in fact claim to have a "natural" right to power, even as tehy disguised thir leadership with a  ~rhetoric of systems, communities and information flow.~
	 ...
	 It was this clain that Stewart Brnad and his colleagues modeled for their clients at the Global Business Network, and it was this claim that the writers of _Wired_ bolstered by depicting subjects such as Esther Dyson and George Gider as people who spoke or acted like computers. As the communcards of the back-to-the-land movement had once argued that they were forerunners of a new, more egalitaritian society on the basis of their being in touch with a shared consciousness, the information consultants of the 1990s asserted that the INternet modeled not only an egalitarian future, but their own, existing lives. In touch witht the flow of information, they could safely represent themselves as a "digital generation" - or, in a term much used at the time, as "digerati".

	 ~The rhetoric of peer-to-peer informationalism, however, much like the rhetoric of consciousness out of which it grew, actively obscures the material and technical infrastructures on which both the Internet and the lives of the digital generation depend. Behind the fantasy of unimpeded information flow lis the reality of millions of plastic keyboards, silicon wafers, glass-faced monitors, and endless miles of cable. All of these technologies depend on manual laborers, first to build them and later to tear them apart. This work remains extraordinarily dangerous, first to those who handle the toxic chemicals required in manufacture and later to those who live on the land, drink the water, and breathe the air into which those chemicals eventually leak.~
** =261=
	 Like the communareds of the 1960s, the techno-utopians of the 1990s denied their dependence on any but themselves. At the same time, they developed a way of thinking and talking about digital technology from within which it was almost impossible to challenge their own elite status.[fn:1] On the communes of the 1960s, the rhetoric of consciousness and community contained little in the way of language with which to describe, let alone confront, a less-than-egalitarian distribution of resources. The same was true of information theory and the universal rhetoric of cybernetics. In both cases, human power was an individual possession, born of the proper use of technologies for the amplification of awareness through access to information. I the writings of the _Wired_group of the 1990s, this model of power and therhetoric on which it depended reappeared. Both persist today throughout discussions of computer mediated communication. Even as they conjured up visions of a disembodied, peer-to-peer utopia, and even as they suggested that such a world would in fact represent a return to a more natural, more intimate state of being, writers such as Kevin Kelly, Ehter Dyson and John Perry Barlow deprived their many readers of a language with which to think about the complex ways in which embodiement shapes all of human life, about the naural and social infrastructures on which that life depends, and about the effects that digital technologies and the network mode of production might have on life and its essential infrastructures.

for these writers, the arrival of the Internet marked not only the end of the industrial era, but the end of history itself. Forty years earlier Stewart Brand and otehrs of his generation had been among the first to come of age in a world that could, as a whole, be destroyed n a matter of minutes. As young adults, although they turned away frm the war-making mind-set, the bureacratic structures, and the partitioned psyches that they imagined characterised life in the military-industrial reserch establishment, many embraced its information theories, its collaborative, experimental orientation, and its underlying world saving mission. Like the atomic scientists at Los Alamos, they would become comprehensive Designers, of their own fates and, by vanguard example, of the fates of mankind. By 1968 more than a few communards beleived, as Steward Brand put it, that "We are as gods and we might as well get good at it."

In his 1968 voluem _The Young Radicals_, Kenneth Kenniston looked on the fractures within the youth movements of the day and wondered how they might ultimately shape American society. "How and whether [the] tension between alienation and activis, is resolved seems to me of the greatest importance," he explained. In the short term, Keniston feared that antiwar activists wuold become frustrated at the failure to stop the conflict in Vietnam and would retreat into academe and the professions. "The field of dissent would be left to the alienated," he wrote, "whose intense quest for _personal_ salvation, meaning, creativity and revelation dulls their perception of the public world and inhibits attempts to better teh lot of others." IN recent years, Keniston's fears seem to have come true, particularly in discussions of the social potential of the Internet and the World Wide Web. To many, these technologies seem to promise what strobe lights and LSD of the Trips Festival once offfered the hippies of the Haight: access to a vision of the patterns underlying the world, and by means of that vision, a way to join one's life to them and to enter a global, harmonious community of mind. As both information technologies and the network mode of production have spread across the landscape, they have been celebrated as sites of personal and collective salvation. And to that extent, they have rendered their believers vulnerable to the material forces of the historical moment in which they live.

And yet, they have preserved a deeper dream as well. As they set off for the hills of New Mexico and Tennessee, the communards of the back-to-the-land movement hoped to build not only communities of consciousness, but real, embodied towns. Most failed- not for lack of good intentions, nor even for lack of tools, but for lack of attention to politics. To the extent that Stewart Brand and the Whole Earth group have succeeeded in linking the ideals of those whom Kenneth keniston called the alienated to digital technologies, ~they have allowed computer users everywhere to imagine their machines as tools of personal liberation.~ O

* Philosophy of code

** 2
This transforms our everyday live into data, a resource to be used by others, usually for profit, which Heidegger terms /standing-reserve[fn:2]/.

** 3
Compters are entangled with our lives n a multitude of different, contradictory and complex ways, providing us with a social milieu that allows us to live in a society that increasingly depends on information and knowledge.

** 4
As software increasingly structure the contemporary world, curiously, it also withdraaws, and becomes hard and harder for us to focus on as itis embedded, hidden, off-shored or merely forgotten about.

** 14
The computational device, as an algorithmic totality, is in a constant state of exception from multiple events which must be attended to, that is, the device is constantly interrupted by a parliament of things or users. THe seemingly end-directed nature of computational processes may actually be constantly deferred internally, that is, never reaching a final goal. In a certain sense, this is an agnostic form of communicative action where devices are in a constant stream of data flow and decision-making which may only occasionally feedback to the human user.
For computer scientists, it is the translation of the continuum in to the discrete that marks the connndition of possibility for computationality... To mediate an object, a computational device requires that it be translated.

** 15
in cutting up the world in this manner, information about the world has to be discarded in order a store a representation within the computer... The other side of the coin, of course, is that these subtractive methods of understanding reality (episteme) produce new knowledges and methods for the control of reality (techne).
For objects in the world to be compuational requires that they offer a certain set of affordances facilitated through the operation of computer code. This is managed through the writing of code that determines certain functions that the software is engineer to perform. These can be at the level of the software itself, and hence invisible to the user directly ( for example application programming interfaces or APIs ), or presented to teh user through a visual interface which allows the user to determine what it does, its /affordance/. To distinguish between the two, it is useful to think of hidden /affordances/ and /visible/ affordances. That is, with visible affordances,
"The valie is clear /on the face of it/... The postbox 'invites' the mailing of a letter, the handle 'wants to be grasped', and things 'tell us what to do with them '(Gibson 1988: 136)"
* Cybernetic Revolutionaries
** 88
The room later broke new ground in interface design, not because of its technical newness but because ot the priority its designers gave to the human operator. "Special attention will be paid to the development of man-machine interfaces," Beer specified, focusing once agian on the user and prioritizing human understanding over technological flashiness. He continued, "The Operations room should be thought of /NOT/ as a room containing interesting bits of equipment /BUT/ as  a control machine comprising men and artifacts in symbiotic relationship. It needs designing as a totality and as an operational entity."(69) The operations room would later emerage as the iconic image of Project Cybersyn and the symbolic heart of the project.
** 91
Scholars such as Lawrence Lessig, Langdon Winner, and Batya Friedman have shown that values can be designed into technologies, meaning that they can uphold certain principles by enabling certain types of behavior while discouraging others.(75)
** 133
   The State Technology Institute had a rather sophisticted understanding of how technological artifacts could uphold particular configurations of power, either by enriching one class at the expense of another or by promoting unjust economic relations between developed and developing nations. Bbut engineers from the State Technology institute did not extend such criticism to the scientific techniques that they used, which they viewed as free of political bias.
** 159-160
he (Beer) concluded that giving workers control of technology, both its use and design, could constitute a new form of worker empowerment.
This assertion differed substantially from how other industrial studies of the day approached the relationship of computer technology and labour in twentieth century production. Such studies, expecially those inspired by Marxist analysis, often presented computers and computer-controlled machinery as tools of capital that automated labor, led to worker deskilling, and gave management greater control of the shop floor. In /Labor and Monopoly capiltal/ (1974), Harry Braverman credits such machinery "as the prime means whereby production may be controlled not by direct producer but by the owner and the representatives of capital" and cites computer technology as routinizing even highly skilled professions such as engineering.
In the 1980s, historian David Noble aslso argued that the introduciton of numerical control technology in factory work stripped workers of their abilities to mentally and physically control factory machninery and gave management greater control of labor. "Because technology is political, it must be recognised that... new technology will invariably constitute extensions of power and control," namely, of managers over workers. That such technologies might be turned to humane ends is a dangerous delusion," he concluded.(54)
In the 1950s Nobert Wiener, author of /Cybernetics/, beleived computers would usher in a second industrial revolution and lead to the creation of an automatic factory. In /The Human Use of Human Beings/(1954) he worries that automated machinery "is the precise economic equivalent of slave labor. Any labor that competes with slave labor must accept the economic conditions of slave labor"(56)
** 161
Beer's proposal bears a close resemblance to the work on participatory design that emerged from the social demoratic governments of Scandanavia in the 1970s. The history of participatory design is often tied to Scandanavian trade union efforts to empower workers during that decade... participatory design used the primacy of management as a starting point then tried to change the dynamics of the labor-capital relationship by changing the social practices surrounding design and use of technology. Initially, this involved educating workers about the technology in use in the workplace so that they could participate in decisions about its use. During the 1980s and 1990s, though, participatory design evolved into a set of methods, theories and practices for involving workers in the design of the computer systems that they used. Proponents of participatory design argued that such practices resulted not only in the creation of better computer systems, in the sense that they better suited workers needs and increaded their ability to get the job done, but also in the creation of more ethical systems that took into account the interests of stakeholders other than management.
** 207 =Important=
But it is important to keep in mind that oftentimes these different interpretations of Cybersyn were not referring to the same system. Although it is tempting top reduce a technological system to its hardware, historians of technology have shown that technological systems are a "seamless web" of social, institutional and technological relationships.(122) Many of the different interpretations of Project cybersyn present in this chapter resulted from Cybersyn's being treated as different sociotechnical systems.
For example, Beer wanted to change ship floor power dynamics by alterning the relationship between workers and technolgists. He wanted to institutionalize a decentralized approach to control by changing how hierachies of command funcitoned within an organisation. And he wanted to change decision-making practices by giving managers access to real-time information, recognizing that the colleciton and transmission of this information depended mostly on human labor. Altering any of tehse social and organizational relations would result in a very different sociotechnical systems from the one Beer proposed. Thus configured, the system could support different configurations of power and different political goals.
** 230
His (Maturana) 1959 work with Jerry Lettvin, Warren McCulloch, and Walter Pitts analyzed the frog's optical system and concluded that what a frog sees is not reality per se but rahter a construction assembled by the frog's visual system. What the frog sees is therefore a product of its biological structure. This distinction formed the foundation for much of Maturana and Varela's  later work in biology and cognition in the 1960s and 1970s, and later inspired the two biologists to break with traditional claims of scientific objectivity and emphasize the role of the observer. One of Maturana's best known claims-"Anything said is siad by an observer"-illustrates this point(21)
** 231 =Flores Computers and Cognition=
/Understanding Computers and Cognition/ begins by critiquing the rationalist assumption that an objective, external world exists. The critique builds on the ideas of Heidegger, Searle, Maturana, J.L Austen, and Hans-Georg Gadamer to show that knowledge is teh result of the interpretation and depends on the past experiences of the interpreter and his or her situation in tradition. Winograd and Flores then argue that because computers lack such experiences and traditions, they cannot replace human beings as knowledge makers. "The ideal of an objectively knowledgable expert must be replaced with a recognition of the importance of background,"(32) Winograd and Flores write. "This can lead to the design of knowledgable community."(32) Moreover, conputer designers should not focus on creating an artifact but should view their labours as a form of "ontological design." Computers should relect who we are and how we interact witht he world, as well as shape what we can do and who we will become... It is now considered a key text inthe field of human-computer interaction.
 fS
* Critical Theory and the Digital
** 1
     The aim is to understand how we can think about computation as part of the social totality and also provide teh means to develop an immanent critique in relation to it. Ther is still mnuch work to be done in humanities and social sciences to understand and critique the computational, and it is a social phenomenon that is accelerating in its growth and ubiquity.

/our societies are increasingly becoming computational, and with it the atttendant tendency of computational systems to reify all aspects of everyday life, it is crucial that we attend to the mechanization of reification and the dangers presented when these processes crystalize into systems , institutions and consciousness itself. The reified world is 'smart', digital and is increasingly colonized by computationally enhanced networks, objects and subjects./

A new /industrial internet/ is emerging, a computational, real-time streaming ecology that is reconfigured in terms of digital flows, fluidities and movement. In the new industrial internet the paradigmatic metaphor I want to use is the real-time
** streaming technologies and the data flows, processual stream-based engines and the computal interfaces that embody them. This is stop thinking about the digital as something static and  instead consider its 'trajectories'. Here I am thinking about the way in which scripts function to create loops and branches, albeit of a highly complex form, and create a stable 'representation', wheich we often think of as a digital 'object'
** .
** 8
noticeable shifts in the mode of production and the mdes of communication increasingly challenge our actual understanding of humanity /qua/ humanity as reflectied in debates over reading skills, neuromarketing, behavioral nudges and so forth. These digital elements are thought to soften the boundaries between human and machine and pose questions for philosophers and theorists about human autonomy and distinctiveness (see Feuler 2011; Steigler 2010)
** 10
the digital is in any ways the creation of a constellation of standards, canonical ways of passing around discrete information and data, that creates what we might call /witnesses/ to the standard - software enforcing the disciplinary action of these standards, such as Application programming Interfaces (APIs). Owning and controlling standards can have a political economic advantage in a post-Fordist society, and much jostling by multinational corperations  and governments is exactly over the imposition of certain kinds of technical standards on the internet, or what Galloway calls protocol.
** 11
   the norms and values of the computational economy can be prescribed quite strongly as a society of control limitng action, thought and even knowledge. This we might understand as the danger of a transition from a rational juridical epistemeology to an /authoritarian-computational/ epistemeology.
** 35
Today, the rise of compuational technolgy in our evryday lives has become a constant theme of modern understandings  of our present situation. However, the salient features identified by the Frankfurt School are also reminiscent of another side of the increasing technological mediation of our lives, namely the interpenetration of computer code and algorithms into our private and public relations with each other; mos so when the code is private or state owned and controlled, without us having access to the contents of these mediating technologies, what I call code-objects or computal objects. These objects contain the logic of behavior, processing, or merely act as gatekeepers and enforcers of a particular form of rationalisation. Similarly, the Frankfurt School sought to map calculative rationalities that emerged in their historical juncture, particularly, instrumental rationality and a tendency towards means-end thinking.
** 37
Today we live in a world of technical beings, whose function and operation are becoming increasingly incterconnected and critical to the lifeworld that we inhabit. Curcially though, this in combined with an increased invisibility or opaqueness of the underlying technologies, and an inability to understand how those systems work, either individually or in concert. This digital world is one of complex, process-orientated computational systems that take on an increasingly complex cognitive heavy-lifting role in society. Without these technologies in place our postmodern financialized economies would doubtlessly collapse - resulting in a crisis of immense proportions. Indeed, our over-reliance on digital technology to moange, control and support many of the aspects of society we now take for granted is predicated on avoiding the kinds of systemic failure and breakdown that occur routinely in computer systems.
** 45
/Bound vs Unbound:/ A notable featuer of digital artifacts is that they tend to be unbound in character. Unlike books, which have clear boundary points marked by the carboard that makes up the covers, digital objects boundaries are drawn by the file format in which they are encoded. This makes it an extremely permeable border, and one that is made of the same digital code that marks the content. Additionally, digital objects are easily networked and aggregated, processed and transcoded into other forms further problematizing boundary points between them. In terms of reading practices, it can be seen that the permeability of boundaries can radically change the reading experience. To some extent the boundlessness of the digital form has been constrained by digital rghts management and related technical protection measures. However, these techniques remain open to hacking techniques and one the locks are brokent he digital content is easily distributed, edited and recombined.
** 46-47
   /Fixed vs. Processional:/ The digital medium facilitates new ways of presenting media that are highly computational. This raises new challenges for scholarship in understanding digital media and the new methods for approaching these media forms. It also raises questions for older humanities that are increasingly accessing their research object through the mediation of processual computational systems, and more particularly through software an dcomputer code. The issue of processual media, in relation to the fixed or time-based media of the twentieth century, is that they incorperate feedback into their media forms, such as interactivity, but also this could be reading speed, colour preferences, etc. in order to change the way a media is oerceived or understood. Digital media are also able to adapt to teh reader/viewer in real-time, changing the content, narrative, structure, presentation and so forth on the fly, as a kind of surveillance literature which is reading the reader as she reads.
** 46
   /Real (physical) versus Digital (virtual):/ This is a common dichotomy that draws some form of dividing line between the so-called real and the so-called digital. Increasingly, with the collapse in our experience of computation from its fixed desktop location, we are using computation in everyday spaces, and which infuses the everyday environment with a computational overlay or layer, such that the computational is not distinct distinct from but deeply embedded in everyday life and hence is post-digital. It is still crucial to historicise the notion of the digital, though, particularly in relation to our changing experience of the digital as previously 'online' and today 'always online', such that being offline increasingly becomes an experience of only the very rich (as choice) or the very poor (by necessity).
** 49									:use:
   But a sociology of culture cannot rest with an analysis of the general relations between types of cultural products... it must also explore in detail the internal structure of cultural forms (the way in which the organisation of society is crystallised in cultural phenomenon) and the mechanisms which determine their reception' (Held 1997: 77)
** 50
   The social dimension of language production and usage is crucially important both to appreciate the way in which the digitial is a social practice that aids interpretation, and the extent to which recent innnnovations in digital techologies like Twitter, Facebook and Google+ lie at the intersection of technology, language and social practice. As Gadamer argues, 'in fact history does not belong ot us, but we belong to it. Long before we understand ourselves through the process of self-examination, we understand ourselves ina  self-evident way in the family, society and state in which we live' (Winogrand and Flores 1987:29)
Digital technologies form a greater part of the technical and media ecology of the environment that surrounds us and records our memories and in some cases /is/ our memories. SO, increasingly, digital media becomes part of our cultural background, and thi contributes to our veery way of experiencing the world nad living and using language.^3

** 59-60
By technicity, Heidegger means more than just technology itself, He uses the term to 'characterize the manenr in which Being manifests itself in the present epoch of the world, accordig to which man experiences the beings around him as objects that can be submitted to his control. It is a consequence of this experience that "technology" becomes possible' (Heidegger 1966, fn 23), For Heidegger, electricity was the paradigmatic metaphor for echnoicity, both in terms of its generation through the challenging forth of nature: through coal, oil, hydroelectric etc., and in terms of the switching systems that were requireed to route produciton, distribution, and consumption of the electricity itself. He saw this switching capacity as a process of ordering by 'ordering beings' where:
Everywhere everything is ordered to standby, to be immediately on hand, indeed, to stand there just so that it may be on call for a further ordering (Heidegger 1977)
** 62
These challenges help us appreciate that to understand the 'digital' requires the concept of the computational to be unpacked. Computational technologies are increasing in their capabilities at an astonishing pace, while our theoretical, political social and legal understanding lags far behind... Indeed, galloway is correct toassert that 'software... asks a question to which the political interpretation is the only coherant answer' (Galloway 2012). However, paradoxically computational systems rely on fairly simple operating logics, and their 'fractal' logic means that techniques form different laters can be reassembled to create new laters and playforms. So, for example, one common distinction that is increasingly used today is between applications (or apps) and files (or data). It is usual for them to be combined together is a so-called wrapper that hides this from the user, for example, in many moblile operating systems, but the basic distinction still reamails as 'pocket' and 'cloud'. In this case, we might say the 'digital' is the user interface (UI) that is experienced by the user, opening the possibility for a critically informed phenomenology of the digital, of the experience generated at the screenic level by the operation of the logistics of computational technology. The digital is then made and remade by the underlying computational system and which conforms to expected practices of the digital, both in terms of the UI and in terms of the user experience.


It is not the '0's and '1's that are in memory chips within the device but the specific modular organisation and deployment of the 'digital' - in its both material and ideological moments, which needs to be considered carefully. Apps are, in this rendering of the representative operation of the UI, the logic and control and are often paid for, and the files are the data or informational context which belongs to the user and is often segregated as a user file area.0 fS
** 94-95
   Thus, as an ontotheology, computationality is a central, effective, increasngly dominant system of meanings and values that become operative and which are not merely abstract but which are organized and lived. Thus computationality cannot be understood at the level of mere opinion of manipulation. It is related to a whole body of compuational practices and excpectations, for example, the assignment of energy towards particular projects, the ordinary uderstanding of the 'nature' of humans, and of the world. This set of meanings and values is experienced as practices which appear as reciprocally confirming, repeated and predictable and also used to describe and understand the world - in some cases, sofware even becomes an explanatory form explanation itself (see Chun 2011). This notion can read through Heidegger, and shares some of the presuppositions and theoretical work undertaken by Horkheimer and Adorno, particularly in relation to the way in which the domination of nature is entangled witht he 'mastery over human nature, the repression of impulse, but also the mastery over other humans' (Schecter 2007:27)

   Today there are rapid changes in social contexts that are made possible by the installation of installation of code/software via computational devices, streams, clouds or networks, what Mitcham (1998:43) calls a 'new ecology of artifice'. The proliferation of computational contrivances that are computationally based is truely breathtaking, and each year there is a large growth in the use of these computational devices and the data they collect. These devices, of course, are not static, nor are they mute, and their interconnections, communications, operation effects and usage are increasingly prescriptive on the everyday life world. But as op pscon psaque devices they are difficult to understand and alalyse due to their staggering rate of change, thanks to the underlying hardware technologues, which are becomng ever smaller, more compact, more powerful and less power-hungry, and also due to the indcrease in complexity , power, range and intelligence of the software that poers these devices. Within the algorithms that power these devices are embedded classificatory schemes and ontologies that pre-strucure the world that is presented. Indeed, this formatting and mediating capacity directly encodes cover concepts into the device.

It should hardly come as a surprise that code/software lies as a mediator between ourselves and our corporeal experiences, disconnecting the physical world from a direct coupling with our physicality, while managing a looser softwarized transmission system. Called 'fly-by-wire' in aircraft design, in reality fly-by-wire is the condition of the computational environment we increasingly experience, hence the term computationality (Berry 2011a). This is a highly mediated existence and has been a growing feature of the (post)modern computational world. While many objects remain firmly material and within our grasp, it is easy to it is easy to see how a more softwarized form of /similacra/ lies just beyond the horizon. Not that software isn't material, of course, certainly it is embedded in physical objects and the physical environment and requires a material carrier to function at all. Nonetheless, the materiality of softwware appears /uncanny/ as a material and therefor can be more difficult to research as a /material/ artefact. This is partly, it has to be said, due to software's increasing tendency to hide it's depths behind glass rectangular shields which yield only to certain prescribed forms of touch-based interfaces. But also because algorithms are always mediated due to their existence as electric pulses and flows within digital circuits. We, therefore, only experience algorithms in their use through practices that rely on computers but also on screenic representation and so forth. Nonetheless, code/software is the paradigmatic case of computationality, and presents us with a research object which is fully located at all major junctures of modern society and is unique nin enabling modern society and in also raising the possibility of reading and understanding the present situation of computationality.
** 96
These devices also enable the assemblage of new social ontologies and the corresponding social epistemologies that we increasingly take for granted in computational society, including Wikipedia, Facebook and Twitter. The extent to which computational devices, and the computational priciples on which they are based and from which hey draw their power, have permiated the way we use and develop knowledges in everyday life is simply breathtaking, had we not already discounted and backgrounded their importance. For example, computational methods like n-gramming are being used to decode everyday life by counting how word usage has changed over time, particularly over a large period of time (Zax 2011). The ability to call up inforamation instantly from a mobile device, combine it with others, subject it to debate and critique through real-time social networks, and then edit, post and dictribute it worldwide would be incredible if it hadn't already started to become so mundane.

Drawing from and extendind Heidegger's concepts we might reconstruct his notions of the mode of technicity and its 'challengin-forth' to one where computationality is central and has a classificatory structure we might call 'streaming-forth'. For Heidegger, 'challengin-forth' is understood as a relationship with the world wherby one treatst he world nature, culture etc. as a vailable for extraction, processing and storing as standing reserve. Thus, this data is 'extracted' without the destruction of the other. This data 'exhaustion' process, where I have called 'streaming-forth' is the creation of information from characteristics, properites and social epistemologies regarding teh object under computational analysis.

Streaming forth generates second order information and data to
maintain a world which is itself /seen/ and /computationally
preocessed/ as flow, but re-articulated within a screeninc form which
produces a universe which is increasingly understood as
object-orientated and discrete. Collected information is processed
together with feedback which creates part of the ecology of
computationality. Adorno calls this an ontological moment, that is the
emregence of a horizon or constellation of key concepts around a
particular historical social formation linked to, in this case,
computational capitalism. We can analyse the history of the changing
forms of human alienation from nature by theorizing epistemeological
questions in relation to socio-economic ethical and political
issues. This is crucial in terms of a certan kind of hstorical
/forgetting/, and reconstruction in computational categories, and even
a forgetting of teh computaitonal as /the/ horizon of
thinking. Indeed, for Adorno, reification 'is as much about
/forgeting/ certain histories as it is aboit exploitation and
preojection' (Schecter 2007: 100).

Computational experience within capitalism is constitutive of this ontology. Additionally, computational devices demonstrate a phenomenological experience of computation, that of the rapid oscillation between the categories Heidegger identified as /Vorhandenheit/Zuhandenheit/ (present-at-hand/ready-to-hand) - and this I call 'glitch' ontology. Thus the computational device constranly changes from being part of the everyday flow of reality, ready-to-hand and the objective 'paused' experience familiar from science, which he calls unready-to-hand, in quick alternation. As Weiser argued,

~Such a disappearance, [in readiness-to-hand] is a fundamental consequence not of technology, but of human psychology. Whenever people learn something sufficiently well, they cease to be aware of it. When you look at a street sign, for example, you absorb its information without conscious performing the act of reading. Computer scientist, economist , and Nobelist herb SImon calls this phenomenon "compiling"; philosopher Michael calls ti the "tacit dimension"; psychologist TK Gibson calls it "visual invariants"; philosophers Georg Gadamer and Martin Heidegger call it "the horizon" and the "ready-to-hand", John Seely Brown at PARC calls it the "periphery". All say, in essence, thato only when things disappear in this was are we freeed to use them without thinking and so to focus beyond them on new goals (Weiser 1991:78)
** 98
** 126
Abduction - one could /abduce/ A from B if A is sufficient (or nearly sufficient) but not necessary for B.

* Science and the Story that We Need by Neil Postman January 1997[fn:3]
 The principal spiritual problem confronting those of us who live in a technological age was spoken of some years ago in a prophetic poem by Edna St. Vincent Millay, in her collection Huntsman, What Quarry?

    Upon this gifted age, in its dark hour,
    Rains from the sky a meteoric shower
    Of facts . . . they lie unquestioned, uncombined.
    Wisdom enough to leech us of our ill
    Is daily spun, but there exists no loom
    To weave it into fabric.
* Question concerning Technology
** 3
Likewise, the essence of technology is by no means anything technological.



* Quotes from Comp Thinking @ stunlaw
One of the striking features of computation is the extent to which forms of pattern matching are required in computer processing. Pattern recognition can be described as a means of identifying repeated shapes or structures which are features of a system under investigation. Whilst we tend to think of patterns as visual, of course they can also be conceptual, iterative, representational, logical, mathematical, etc. in form providing the underlying computational system can be programmed to recognise the distinctive shape of the pattern from the data. They can also consist of meta-patterns as described by Gregory Bateson as patterns that can detected across different spheres, such as culture, humanities, science and the social or 'the pattern that connects' (see Bateson 1979; Dixon 2012).


The recognition of patterns and uncovering their relationships in sets of data was called 'abductive reasoning' by Charles Peirce, who contrasted it with inductive and deductive reasoning. Indeed, Peirce described abduction as a kind of logical inference akin to guessing. This he called the leap of abduction where by one could abduce A from B if A is sufficient (or nearly sufficient) but not necessary for B. The possible uses of this within a computational context should be fairly obvious, especially when software is handling partial, fuzzy or incomplete data and needs to generate future probabilistic decision points, or recognise important features or contours in a data set./
For Peirce, abduction works from these surprising facts to determine a possible, plausible explanation.  Furthermore, Peirce stresses the fact that the logic of abduction is fallible – abductive inferences, like induction, can, and do, lead us to the wrong result (Pearce 1958 5.189, 5.197, 6.532).  However, as a part of the triad, abduction is able to correct itself, once it is investigated by deduction and tested by induction (Pearce 1958 5.574).  Because of this, we should never take the conclusion of an abductive inference to be a fact in and of itself until it is tested.  Until that point “abduction commits us to nothing…it merely causes a hypothesis to be set down upon our docket of cases to be tried” (Pearce 1958 5.602).  Furthermore, by hypothesis, Peirce does not just mean scientific hypotheses.  Abduction certainly includes the more formalized, conscious cognitive process of deliberately searching for an explanation to a set of particular facts; however, abduction is also a logical inference used in everyday life from crude hypotheses (his Catholic priest example) to perceptual judgments (understanding the information that we receive from our senses) (Pearce 1958 7.202, 5.180, 5.184) (Eldridge n.d.).

Within computer science, and particularly related to the more micro level problem of recognising patterns themselves within data sets automatically using computation, is an important and challenging area of research. The main forms of pattern recognition (we can think of these as patterns to find patterns) used in computation are usually enumerated as template-matching, prototype matching, feature analysis, recognition by components, fourier analysis, and lastly bottom-up and top-down processing. I'll briefly describe each of the six main approaches.

Template Matching: This is where a computational device uses a set of images (or templates) against which it can compare a data set, which might be an image for example (for examples of an image set, see Cole et al. 2004).

Template Matching (Jahangir 2008)
Prototype Matching: This form of patten matching uses a set of prototypes, which are understood as an average characteristic of a particular object or form. The key is that there does not need to be a perfect match merely a high probability of likelihood that the object and prototype are similar (for an example, see Antonina et al. 2003).

Feature Analysis: In this approach a variety of approaches are combined including detection, pattern dissection, feature comparison, and recognition. Essentially the source data is broken into key features or patterns to be compared with a library of partial objects to be matched with (for examples, see Morgan n.d.).

Recognition by Components: In this approach objects are understood to be made up of what are called 'geons' or geometric primitives. A sample of data or images is then processed through feature detectors which are programmed to look for curves, edges, etc. or through a geo detector which looks for simple 2D or 3D forms such as cylinders, bricks, wedges, cones, circles, and rectangles (see Biederman 1987).

Fourier Analysis: This form of pattern matching uses algorithms to decompose something into smaller pieces which can then be selectively analysed. This decomposition process itself is called the Fourier transform.  For example, an image might be broken down into a set of twenty squares across the image field, each of which being smaller, is made faster to process. As Moler (2004) argues, 'we all use Fourier analysis every day without even knowing it. Cell phones, disc drives, DVDs, and JPEGs all involve fast finite Fourier transforms'. Fourier transformation is also used to generate a compact representation of a signal. For example, JPEG compression uses a variant of the Fourier transformation (discrete cosine transform) of small square pieces of the digital image. The Fourier components of each square are then rounded to lower arithmetic precision, and weak components are discarded, so that the remaining components can be stored in much less computer memory or storage space. To reconstruct the image, each image square is reassembled from the preserved approximate Fourier-transformed components, which are then inverse-transformed to produce an approximation of the original image, this is why the image can produce 'blocky' or the distinctive digital artefacts in the rendered image, see JPEG (2012).

Bottom-up and Top-down Processing: Finally, in the Bottom-up and Top-down methods an interpretation emerges from the data, this is called data-driven or bottom-up processing. Here the interpretation of a data set to be determined mostly by information collected, not by your prior models or structures being fitted to the data, hence this approach looks for repeated patterns that emerge from the data. The idea is that starting with no knowledge the software is able to learn to draw generalisations from particular examples. Alternatively an approach where prior knowledge or structures are applied data is fitted into these models to see if there is a 'fit'. This approach is sometimes called schema-driven or top-down processing. A schema is a pattern formed earlier in a data set or drawn from previous information (Dewey 2011).

What should be apparent from this brief discussion of the principles of abduction and pattern-matching in computer science is their creative possibilities for generating results from data sets. The ability to generate  hypothesises on the basis of data, which is fallible and probabilistic allows for computational devices to generate forecasts and predictions based on current and past behaviours, data collection, models, and images. It is this principle of abductive reason which makes computational reasoning different from instrumental reason, and particularly from the iron-cage of logical implication or programmatic outcome that instrumental reason suggests. Indeed Alexander that the most useful patterns are generative,

    These patterns in our minds are, more or less, mental images of the patterns in the world: they are abstract representations of the very morphological rules which define the patterns in the world. However, in one respect they are very different. The patterns in the world merely exist. But the same patterns in our minds are dynamic. They have force. They are generative. They tell us what to do; they tell us how we shall, or may, generate them; and they tell us too, that under certain circumstances, we must create them. Each pattern is a rule which describes what you have to do to generate the entity which it defines. (Alexander 1979: 181-182)
* Footnotes

[fn:2] Heid 1993a: 322

[fn:3] http://www.firstthings.com/article/1997/01/003-science-and-the-story-that-we-need
